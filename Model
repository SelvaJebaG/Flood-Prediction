import pandas as pd 
import numpy
import numpy as np 
import matplotlib.pyplot as plt
import math
from keras.models import Sequential
import tensorflow
import tensorflow as tf
from tensorflow.keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional,Conv1D, Flatten, SimpleRNN, Activation
from sklearn.preprocessing import MinMaxScaler
from sklearn import metrics
from sklearn.metrics import mean_absolute_error as mae
from sklearn.metrics import mean_squared_error as mse
from keras import Input # for instantiating a keras tensor
from keras.layers import Bidirectional, GRU, RepeatVector, Dense, TimeDistributed # for creating layers inside the Neural Network
df=pd.read_csv('Data.csv',index_col='Date',parse_dates=True)
df
dataset=df.copy()
from statsmodels.tsa.stattools import adfuller
def test_stationarity(timeseries):
    
    #Determing rolling statistics
    rolmean = pd.rolling_mean(timeseries, window=12)
    rolstd = pd.rolling_std(timeseries, window=12)

    #Plot rolling statistics:
    orig = plt.plot(timeseries, color='blue',label='Original')
    mean = plt.plot(rolmean, color='red', label='Rolling Mean')
    std = plt.plot(rolstd, color='black', label = 'Rolling Std')
    plt.legend(loc='best')
    plt.title('Rolling Mean & Standard Deviation')
    plt.show(block=False)
    
    #Perform Dickey-Fuller test:
    print ('Results of Dickey-Fuller Test:')
    dftest = adfuller(timeseries, autolag='AIC')
    dfoutput = pd.Series(dftest[0:8], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])
    for key,value in dftest[4].items():
        dfoutput['Critical Value (%s)'%key] = value
    print (dfoutput)
series = dataset['Actual Rainfall(mm)'].values
series
result = adfuller(series, autolag='AIC')
#Extracting the values from the results:

print('ADF Statistic: %f' % result[0])

print('p-value: %f' % result[1])

print('Critical Values:')

for key, value in result[4].items():
    print('\t%s: %.3f' % (key, value))
if result[0] < result[4]["5%"]:
    print ("Reject Ho - Time Series is Stationary")
else:
    print ("Failed to Reject Ho - Time Series is Non-Stationary")
#ACF and PACF plots:
from statsmodels.tsa.stattools import acf, pacf
lag_acf = acf(tstrain, nlags=20)
lag_pacf = pacf(tstrain, nlags=20, method='ols')
#Plot ACF: 
plt.subplot(121) 
plt.plot(lag_acf)
plt.axhline(y=0,linestyle='--',color='gray')
plt.axhline(y=-1.96/np.sqrt(len(tstrain)),linestyle='--',color='gray')
plt.axhline(y=1.96/np.sqrt(len(tstrain)),linestyle='--',color='gray')
plt.title('Autocorrelation Function')
#Plot PACF:
plt.subplot(122)
plt.plot(lag_pacf)
plt.axhline(y=0,linestyle='--',color='gray')
plt.axhline(y=-1.96/np.sqrt(len(tstrain)),linestyle='--',color='gray')
plt.axhline(y=1.96/np.sqrt(len(tstrain)),linestyle='--',color='gray')
plt.title('Partial Autocorrelation Function')
plt.tight_layout()
from __future__ import print_function
import numpy as np
from scipy import stats
import pandas as pd
import matplotlib.pyplot as plt

import statsmodels.api as sm

from statsmodels.graphics.api import qqplot
fig = plt.figure(figsize=(12,8))
ax1 = fig.add_subplot(211)

fig = sm.graphics.tsa.plot_acf(tstrain.values.squeeze(), lags=40, ax=ax1)
ax2 = fig.add_subplot(212)
fig = sm.graphics.tsa.plot_pacf(tstrain, lags=40, ax=ax2)
dataset = dataset.astype('float32')
# normalize the dataset
scaler = MinMaxScaler(feature_range=(0, 1))
dataset = scaler.fit_transform(dataset)
# split into train and test sets
train_size = int(len(dataset) * 0.80)
test_size = len(dataset) - train_size
train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]
print(len(train), len(test))
print((train.shape), (test.shape))
# convert an array of values into a dataset matrix
def create_dataset(dataset, look_back=1):
	dataX, dataY = [], []
	for i in range(len(dataset)-look_back-1):
		a = dataset[i:(i+look_back), 0]
		dataX.append(a)
		dataY.append(dataset[i + look_back, 0])
	return numpy.array(dataX), numpy.array(dataY)
# reshape into X=t and Y=t+1
look_back = 1
x_train, y_train = create_dataset(train, look_back)
x_test, y_test = create_dataset(test, look_back)
# reshape input to be [samples, time steps, features]
x_train = numpy.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1]))
x_test = numpy.reshape(x_test, (x_test.shape[0], 1, x_test.shape[1]))
model = Sequential()
model.add(Conv1D(256,2, padding='same', activation='elu',input_shape=(1, look_back)))
model.add(Bidirectional(GRU(32, activation="relu", return_sequences='true')))
model.add(Bidirectional(GRU(8, activation="relu", return_sequences='true')))
model.add(Dense(128, activation='elu'))
model.add(Flatten())
model.add(Dense(1, activation='linear'))
#optimizer
optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9, epsilon=None, decay=0.0)
model.compile(loss='mean_squared_error', optimizer='adamax', metrics=['mae'])
model.summary()
model.fit(x_train, y_train, epochs=50, batch_size=100, verbose=1)
from sklearn import metrics
from sklearn.metrics import mean_absolute_error as mae
from sklearn.metrics import mean_squared_error as mse
train_predict = model.predict(x_train)
r2 = metrics.r2_score(y_train, train_predict)
print('Training Mean Absolute Error:', mae(y_train, train_predict))
print('Training Mean Squared Error:', mse(y_train, train_predict))
print('Training Root Mean Squared Error:', np.sqrt(mse(y_train, train_predict)))
print('Training R Squared:', r2)
test_predict = model.predict(x_test)
r2 = metrics.r2_score(y_test, test_predict)
print('Testing Mean Absolute Error', mae(y_test, test_predict))
print('Testing Mean Squared Error:', mse(y_test, test_predict))
print('Testing Root Mean Squared Error:', np.sqrt(mse(y_test, test_predict)))
print('Testing R Squared:', r2)



